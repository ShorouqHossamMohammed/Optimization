{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2166176",
   "metadata": {},
   "source": [
    "# Multivariable Linear Regression Model Training Using Momentum-based and Vanilla GD with Batch/mini-Batch/Stochastic Variants:\n",
    "## Please read and follow the instructions very carefully (Good Luck).\n",
    "### In this task you should implement a mini-batch momentum based gradient descent optimizer to train a multivariable linear regression model.\n",
    "### Your optimizer should be able to work as Stochastic/mini-batch/Batch by just adjusting mini-batch size without any code modification.\n",
    "### Your optimizer should also work as GD without momentum by adjusting the hyperparameter value (think how?) without any code modification.\n",
    "### Make your implementation as a function.\n",
    "### The function should return the model hyperparameters and required output for plotting the learning curves.\n",
    "### Data shuffle and adding the first bias feature (containig ones) must be berformed inside the function.\n",
    "### The optimizer must be able to deal with any mini-batch size.\n",
    "### Maximum number of epochs must be stated in order to avoid infinite loop.\n",
    "### Gradient check and cost convergence check stop criterias must be implemented.\n",
    "### You must plot the following learning curves:\n",
    "#### - Loss vs. iterations (not Epoch).\n",
    "#### - Loss vs. theta0, loss vs. theta1, loss vs. theta2.\n",
    "### You must evaluate your model using r2_score metrics and achieve at least 0.8 score in any scenario of your choice and plot all the learning curves.\n",
    "### You must run (at least) the following scenarios and show results and learning curves for each scenario (Do not do new implementation for each scenario. Use your implemenation and change input parameter values of the function):\n",
    "#### - Momentum based with mini-batch size of your choice.\n",
    "#### - Momentum based with Stochastic Gradient Descent.\n",
    "#### - Momentum based with Batch Gradient Descent.\n",
    "#### - Vanilla GD with mini-batch size of your choice.\n",
    "#### - Vanilla GD with Stochastic Gradient Descent.\n",
    "#### - Vanilla GD with Batch Gradient Descent.\n",
    "#### - Any scenario with mini-batch size 32.\n",
    "### You must use vectorize implementation. i.e., you should not have only two loops one for epochs and one for iterations (Only optimizer's loops).\n",
    "### Your function should take the following inputs:\n",
    "#### Input features, target label, learning rate, momentum term, mini-batch size, max number of epochs, gradient check tolerence, cost convergence check tolerence, and any other argument you think useful.\n",
    "### The function should initialize the model parameters to zeros.\n",
    "### Generate a regression data with two input features and 500 observations (You can see a sample code below for data generation. feel free to change any of the values to btain your own data to achieve the required score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7df2851f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.123620</td>\n",
       "      <td>6.792647</td>\n",
       "      <td>954.178622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.852143</td>\n",
       "      <td>6.144385</td>\n",
       "      <td>1044.922138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.195982</td>\n",
       "      <td>5.238110</td>\n",
       "      <td>1056.457756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.795975</td>\n",
       "      <td>7.255180</td>\n",
       "      <td>978.249228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.468056</td>\n",
       "      <td>6.738925</td>\n",
       "      <td>926.832960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2            y\n",
       "0  3.123620  6.792647   954.178622\n",
       "1  4.852143  6.144385  1044.922138\n",
       "2  4.195982  5.238110  1056.457756\n",
       "3  3.795975  7.255180   978.249228\n",
       "4  2.468056  6.738925   926.832960"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Generate random data\n",
    "np.random.seed(42) \n",
    "\n",
    "# Create 500 records\n",
    "n_samples = 500\n",
    "\n",
    "# Independent variables (features)\n",
    "x1 = np.random.uniform(low=2.0, high=5.0, size=n_samples)\n",
    "x2 = np.random.uniform(low=4.0, high=8.0, size=n_samples)\n",
    "\n",
    "# Dependent variable (target)\n",
    "y = 1000 + 50 * x1 - 30 * x2 + np.random.normal(loc=0, scale=10, size=n_samples)\n",
    "\n",
    "# Create a DataFrame\n",
    "data = {\n",
    "    'x1': x1,\n",
    "    'x2': x2,\n",
    "    'y': y\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df.head()  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f105c3cb",
   "metadata": {},
   "source": [
    "# Your submission must be in .ipynb file format that contains all results and curves.\n",
    "# Do not submit any link for a notebook.\n",
    "# Do not forget to klick on Hand In after uploading your file.\n",
    "# Good Luck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8339ee36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df0e5da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 2.25604239e+00, 7.68999752e+00, 8.90604411e+02],\n",
       "       [1.00000000e+00, 2.28332888e+00, 6.57599817e+00, 9.10496881e+02],\n",
       "       [1.00000000e+00, 3.15830791e+00, 6.23856133e+00, 9.70914346e+02],\n",
       "       ...,\n",
       "       [1.00000000e+00, 2.35645375e+00, 6.95213447e+00, 9.25542620e+02],\n",
       "       [1.00000000e+00, 4.43034018e+00, 7.38580917e+00, 1.00390077e+03],\n",
       "       [1.00000000e+00, 3.10334940e+00, 5.42438690e+00, 9.79632819e+02]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data=np.hstack((np.ones((df.shape[0], 1)),df))\n",
    "np.random.shuffle(Data)\n",
    "Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf3585b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=Data[:,:-1]\n",
    "y_true=Data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58ba572",
   "metadata": {},
   "source": [
    "Input features, target label, learning rate, momentum term, mini-batch size, max number of epochs, gradient check tolerence, cost convergence check tolerence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7ce6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Momentum_Based_Multi_Variable_GD(X, y, learning_rate, momentum_term,batch_size, beta,max_epoch,gradient_tolerence,convergence_tolerence):\n",
    "    all_costs = []\n",
    "    all_theta = []\n",
    "    # Initialize theta vector with zeros\n",
    "    thetas = np.zeros((X.shape[1], 1))\n",
    "    # Initialize velocity vector with zeros\n",
    "    v_t = np.zeros((X.shape[1], 1))\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        print(f\"****************Iteration {i}*******************\")\n",
    "\n",
    "        # Calculate hypothesis\n",
    "        h = X @ thetas\n",
    "\n",
    "         #  for plot thetas with loss later\n",
    "        all_theta.append(thetas.flatten().tolist())\n",
    "\n",
    "        # Calculate error vector\n",
    "        error_vector = h - y\n",
    "\n",
    "        # Calculate cost function\n",
    "        s = error_vector.T @ error_vector\n",
    "        E = s / (2 * len(y))\n",
    "        costs.append(E)\n",
    "        print(\"Cost (J):\", E)\n",
    "\n",
    "        # Calculate gradient vector\n",
    "        gradient_vector = (1 / len(X)) * (X.T @ (h - y))\n",
    "\n",
    "        # Calculate norm of the gradient vector\n",
    "        gradient_vector_norm = np.linalg.norm(gradient_vector)\n",
    "        print(\"Gradient Vector Norm:\", gradient_vector_norm)\n",
    "\n",
    "        # Gradient Check\n",
    "        if gradient_vector_norm < 0.001:\n",
    "            break\n",
    "\n",
    "        # convergence check\n",
    "        if i > 0 and (abs(costs[i-1] - costs[i]) < 0.001):\n",
    "            break\n",
    "\n",
    "        # Update velocity\n",
    "        v_theta = beta * v_theta + (1 - beta) * gradient_vector\n",
    "\n",
    "        # Update thetas\n",
    "        thetas = thetas - learning_rate * v_theta\n",
    "        print(\"Updated Thetas:\", thetas)\n",
    "\n",
    "    all_theta = np.array(all_theta)\n",
    "\n",
    "    print(\"*************Training Report************\\n\")\n",
    "    print(f\"Gradient Descent converged after {i} iterations\\n\")\n",
    "    print(\"Final Cost (J):\", E)\n",
    "    print(\"Final Thetas:\", thetas)\n",
    "    print(\"Final Hypothesis (h(X)):\", h)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return thetas, h, costs, all_theta\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015a3347",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Momentum_Based_Multi_Variable_GD(X, y, learning_rate, momentum_term,batch_size, beta,max_epoch,gradient_tolerence,convergence_tolerence):\n",
    "    \n",
    "    all_costs = []\n",
    "    all_theta = []\n",
    "    # Initialize theta vector with zeros\n",
    "    thetas = np.zeros((X.shape[1], 1))\n",
    "    # Initialize velocity vector with zeros\n",
    "    v_t = np.zeros((X.shape[1], 1))\n",
    "\n",
    "    for i in range(Max_iter):\n",
    "        print(f\"*********** Iteration {i} ***********************\")\n",
    "\n",
    "        for m in range(0, len(x), batch_size):\n",
    "            \n",
    "            all_theta.append(theta.flatten().tolist())\n",
    "\n",
    "\n",
    "            \n",
    "            x_batch = x[m:m+batch_size]\n",
    "            y_batch = y[m:m+batch_size]\n",
    "\n",
    "            h = x_batch @ theta\n",
    "            \n",
    "\n",
    "            \n",
    "            error = h - y_batch\n",
    "            norm_error = np.linalg.norm(error)\n",
    "            norm_squar = norm_error ** 2\n",
    "            E = norm_squar / (2 * len(x_batch))\n",
    "            all_costs.append(E)\n",
    "\n",
    "            \n",
    "            gradient_vector = (x_batch.T @ error) / len(x_batch)\n",
    "\n",
    "           \n",
    "\n",
    "            \n",
    "            # Update v_theta\n",
    "            v_t = beta * v_t + (1 - beta) * gradient_vector\n",
    "\n",
    "            # Update thetas\n",
    "            thetas = thetas - learning_rate * v_t\n",
    "\n",
    "\n",
    "        print(\"*************Training Report************\\n\")\n",
    "        print('j =', E, \"\\n\")\n",
    "        print('h(x):', h)\n",
    "        print('Error Vector:\\n', error, \"\\n\")\n",
    "        Gradient_norm = np.linalg.norm(gradient_vector)\n",
    "        print('Gradient Vector Norm :', Gradient_norm, \"\\n\")\n",
    "      \n",
    "\n",
    "        # Gradient Check:\n",
    "        if Gradient_norm <gradient_tolerence:\n",
    "            break\n",
    "\n",
    "        # convergence check:\n",
    "        if i > 0 and abs(all_costs[-int((len(x)/batch_size)+1)] - all_costs[-1]) < convergence_tolerence1:\n",
    "            break\n",
    "\n",
    "        print(f\"v_t{i} : \", V_t, \"\\n\")\n",
    "        print(\"theta_updated :\", theta, \"\\n\")\n",
    "\n",
    "    \n",
    "    all_theta = np.array(all_theta)\n",
    "\n",
    "    return theta, h, all_costs, all_theta\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
